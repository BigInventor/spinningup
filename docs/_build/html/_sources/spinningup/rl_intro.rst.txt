==========================
第一部分：强化学习中的核心概念
==========================


.. contents:: 目录
    :depth: 2

欢迎来到强化学习的介绍！我们希望你能了解以下内容：

* 常见的符号表示
* 高层次的讲解：关于强化学习算法究竟能做什么（我们会尽量避免 *如何做* 这个话题）
* 算法背后的核心数学知识

总的来说，强化学习是关于智能体以及它们如何通过反复试错来学习的研究。它定义了通过奖励或者惩罚智能体的动作，从而使它未来更容易重复或者放弃某一动作的思想。

强化学习能做什么
===============

基于强化学习的方法已经在很多地方取得了成功。例如，它被用来教电脑在仿真环境下控制机器人：

.. raw:: html

    <video autoplay="" src="https://storage.googleapis.com/joschu-public/knocked-over-stand-up.mp4" loop="" controls="" style="display: block; margin-left: auto; margin-right: auto; margin-bottom:1.5em; width: 100%; max-width: 720px; max-height: 80vh;">
    </video>

以及在现实世界中：

.. raw:: html

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
        <iframe src="https://www.youtube.com/embed/jwSbzNHGflM?ecver=1" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
    </div>
    <br />

强化学习因为在复杂策略游戏中有了突破而名声大噪，最著名的要数 `围棋`_ 、 `Dota`_ 、教电脑 `玩Atari游戏`_ 以及训练模拟机器人 `听从人类的指令`_ 。

.. _`围棋`: https://deepmind.com/research/alphago/
.. _`Dota`: https://blog.openai.com/openai-five/
.. _`玩Atari游戏`: https://deepmind.com/research/dqn/
.. _`听从人类的指令`: https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/


核心概念和术语
============================

.. figure:: ../images/rl_diagram_transparent_bg.png
    :align: center
    
    智能体和环境的循环作用

强化学习的主要角色是 **智能体** 和 **环境** ,环境是智能体存在和互动的世界。智能体在每一步的交互中，都会获得对于所处环境状态的观察（有可能只是一部分），然后决定下一步要执行的动作。环境会因为智能体对它的动作而改变，也可能自己改变。

智能体也会从环境中感知到 **奖励** 信号，一个表明当前状态好坏的数字。智能体的目标是最大化累计奖励，也就是 **回报** 。强化学习就是智能体通过学习来完成目标的方法。

为了便于后面的学习，我们介绍一些术语，以便了解强化学习能做什么：

* 状态和观察(states and observations)
* 动作空间(action spaces)
* 策略(policies)
* 行动轨迹(trajectories)
* 不同的回报公式(formulations of return)
* 强化学习优化问题(the RL optimization problem)
* 值函数(value functions)

状态和观察
-----------------------

一个 **状态** :math:`s` 是一个关于这个世界状态的完整描述。这个世界除了状态意外没有隐藏的信息。而 **观察** :math:`o` 是对于一个状态的部分描述，可能会漏掉一些信息。

在深度强化学习中，我们一般用 `实数向量、矩阵或者更高阶的张量（tensor）`_ 表示状态和观察。比如说，视觉上的 *观察* 可以用RGB矩阵的方式表示其像素值；机器人的 *状态* 可以通过关节角度和速度来表示。

如果智能体能够对于环境的所有状态有全面的观察，我们通常说环境是被 **全面观察** 的。如果智能体只能观察到一部分，我们称之为 **部分观察**。

.. admonition:: 你应该知道

    强化学习有时候用这个符号 :math:`s` 代表状态 , 有些地方也会写作观察符号 :math:`o`.  尤其是：当智能体在决定采取什么动作的时候：我们会用符号表示动作是基于状态的，但实际上，动作是基于观察的，因为智能体并不知道状态（只能通过观察了解状态）。

    在我们的教程中，我们会按照标准使用符号，不过你一般能从上下文中看出来具体是什么意思。如果你对有些内容不清楚，请提出issue！我们的目的是教会大家，不是让大家混淆。

.. _`实数向量、矩阵或者更高阶的张量（tensor）`: https://en.wikipedia.org/wiki/Real_coordinate_space

动作空间
-------------

不同的环境有不同的动作。所有有效动作的集合称之为 **动作空间**。有些环境，比如说 Atari 游戏和围棋，具有的是 **离散动作空间**，这种情况下智能体只能采取有限的动作。其他的一些环境，比如智能体在物理世界控制机器人，属于 **连续动作空间**。在连续动作空间中，动作是实数向量。

这种区别对于深度强化学习影响很大。有些种类的算法只能用在某一些案例上，如果用在别的地方可能需要重写大量代码。

策略
--------

**策略** 是智能体用于决定下一步执行什么行动的规则。可以是确定性的，一般表示为：:math:`\mu`:

.. math::

    a_t = \mu(s_t),

也可以是随机的，一般表示为 :math:`\pi`:

.. math::

    a_t \sim \pi(\cdot | s_t).

因为策略就是智能体的大脑，所以很多时候“策略”和“智能体”这两个名词经常互换，例如：“策略的目的是最大化奖励”。

在深度强化学习中，我们处理的是参数化的策略，这些策略的输出，依赖于一系列计算函数，而这些函数又依赖于参数（例如神经网络的权重和误差），所以我们可以通过一些优化算法改变智能体的的行为。

我们经常把这些策略的参数写作 :math:`\theta` 或者 :math:`\phi` ，然后把它写在策略的下标上来强调两者的联系。

.. math::

    a_t &= \mu_{\theta}(s_t) \\
    a_t &\sim \pi_{\theta}(\cdot | s_t).


确定性策略
^^^^^^^^^^^^^^^^^^^^^^

**例子：确定性策略：** 这是一个基于 TensorFlow 在连续动作空间上确定性策略的简单例子：

.. code-block:: python

    obs = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)
    net = mlp(obs, hidden_dims=(64,64), activation=tf.tanh)
    actions = tf.layers.dense(net, units=act_dim, activation=None)

其中， ``mlp`` 是一个给定大小和激活函数，把多个 ``密集层`` （dense layer）相互堆积在一起的函数

随机性策略
^^^^^^^^^^^^^^^^^^^

深度强化学习中最常见的两种随机策略是 **categorical policies** 和 **diagonal Gaussian policies**. 

`Categorical`_ 策略适用于离散行动空间，而
`Gaussian`_ 策略一般用在连续行动空间

使用和训练随机策略的时候有两个重要的计算：

* 从策略中采样行动
* 计算特定行为的似然(likelihoods) :math:`\log \pi_{\theta}(a|s)`.


下面我们介绍一下这两种策略


.. admonition:: Categorical Policies

    A categorical policy is like a classifier over discrete actions. You build the neural network for a categorical policy the same way you would for a classifier: the input is the observation, followed by some number of layers (possibly convolutional or densely-connected, depending on the kind of input), and then you have one final linear layer that gives you logits for each action, followed by a `softmax`_ to convert the logits into probabilities. 

    **采样** 给定每个行动的可能性，TensorFlow之类的框架有内置采样工。具体可查阅
`tf.distributions.Categorical`_  或 `tf.multinomial`_ 的文档。

    **Log-Likelihood.** Denote the last layer of probabilities as :math:`P_{\theta}(s)`. It is a vector with however many entries as there are actions, so we can treat the actions as indices for the vector. The log likelihood for an action :math:`a` can then be obtained by indexing into the vector:

    .. math::

        \log \pi_{\theta}(a|s) = \log \left[P_{\theta}(s)\right]_a.


.. admonition:: Diagonal Gaussian Policies

    A multivariate Gaussian distribution (or multivariate normal distribution, if you prefer) is described by a mean vector, :math:`\mu`, and a covariance matrix, :math:`\Sigma`. A diagonal Gaussian distribution is a special case where the covariance matrix only has entries on the diagonal. As a result, we can represent it by a vector.

    A diagonal Gaussian policy always has a neural network that maps from observations to mean actions, :math:`\mu_{\theta}(s)`. There are two different ways that the covariance matrix is typically represented.

    **The first way:** There is a single vector of log standard deviations, :math:`\log \sigma`, which is **not** a function of state: the :math:`\log \sigma` are standalone parameters. (你应该知道: our implementations of VPG, TRPO, and PPO do it this way.)

    **The second way:** There is a neural network that maps from states to log standard deviations, :math:`\log \sigma_{\theta}(s)`. It may optionally share some layers with the mean network.

    Note that in both cases we output log standard deviations instead of standard deviations directly. This is because log stds are free to take on any values in :math:`(-\infty, \infty)`, while stds must be nonnegative. It's easier to train parameters if you don't have to enforce those kinds of constraints. The standard deviations can be obtained immediately from the log standard deviations by exponentiating them, so we do not lose anything by representing them this way.

    **Sampling.** Given the mean action :math:`\mu_{\theta}(s)` and standard deviation :math:`\sigma_{\theta}(s)`, and a vector :math:`z` of noise from a spherical Gaussian (:math:`z \sim \mathcal{N}(0, I)`), an action sample can be computed with

    .. math::

        a = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot z,

    where :math:`\odot` denotes the elementwise product of two vectors. Standard frameworks have built-in ways to compute the noise vectors, such as `tf.random_normal`_. Alternatively, you can just provide the mean and standard deviation directly to a `tf.distributions.Normal`_ object and use that to sample.

    **Log-Likelihood.** The log-likelihood of a :math:`k` -dimensional action :math:`a`, for a diagonal Gaussian with mean :math:`\mu = \mu_{\theta}(s)` and standard deviation :math:`\sigma = \sigma_{\theta}(s)`, is given by

    .. math::

        \log \pi_{\theta}(a|s) = -\frac{1}{2}\left(\sum_{i=1}^k \left(\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2 \log \sigma_i \right) + k \log 2\pi \right).

.. _`Categorical`: https://en.wikipedia.org/wiki/Categorical_distribution
.. _`Gaussian`: https://en.wikipedia.org/wiki/Multivariate_normal_distribution
.. _`softmax`: https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax
.. _`tf.distributions.Categorical`: https://www.tensorflow.org/api_docs/python/tf/distributions/Categorical
.. _`tf.multinomial`: https://www.tensorflow.org/api_docs/python/tf/multinomial
.. _`tf.random_normal`: https://www.tensorflow.org/api_docs/python/tf/random_normal
.. _`tf.distributions.Normal`: https://www.tensorflow.org/api_docs/python/tf/distributions/Normal

运动轨迹
------------

运动轨迹 :math:`\tau` 指的是状态和行动的序列。

.. math::

    \tau = (s_0, a_0, s_1, a_1, ...).

第一个状态 :math:`s_0`，是从 **开始状态分布** 中随机采样的，有时候表示为 :math:`\rho_0`:
.. math::

    s_0 \sim \rho_0(\cdot).

转态转换（从某一状态时间 :math:`t`, :math:`s_t`到另一状态时间 :math:`t+1`, :math:`s_{t+1}` 会发生什么），是由环境的自然法则确定的，并且只依赖于最近的行动 :math:`a_t`。它们可以是确定性的：

.. math::

    s_{t+1} = f(s_t, a_t)

而可以是随机的：

.. math::

    s_{t+1} \sim P(\cdot|s_t, a_t).

智能体的行为由策略确定。

.. admonition:: 你应该知道

    行动轨迹常常也被称作 **episodes** 或者 **rollouts**。


奖励和返回
-----------------

奖励函数在强化学习中非常重要。它依赖于这个世界的状态，已经做出的行动，以及世界下一步的行动。

.. math::

    r_t = R(s_t, a_t, s_{t+1})

有时候这个公式会被改成只依赖于当前的状态 :math:`r_t = R(s_t)`，或者状态和行动对 :math:`r_t = R(s_t,a_t)`。

智能体的目标是最大化行动轨迹的累计奖励，这意味着很多事情。我们会把所有的情况表示为 :math:`R(\tau)`，要么可以很清楚的从上下文看出来，要么就不重要。（因为相同的方程式适用于所有情况。）

**有限无衰减收益**，指的是在一个固定窗口步数内获得的累计奖励：

.. math::

    R(\tau) = \sum_{t=0}^T r_t.

另一种叫做 **无限衰减收益**，指的是智能体获得的全部奖励之和，但是奖励会因为获得的时间不同而打折（乘上一个系数）。这个公式包含一个衰减率 :math:`\gamma \in (0,1)`:

.. math::

    R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t.

这里为什么要加上一个衰减率呢？为什么不直接把所有的奖励加在一起？可以从两个角度来解释： 直观上讲，现在的奖励比外来的奖励要好，所以未来的奖励会打折；数学角度上，无限多个奖励的和很可能 `不收敛`_ ，有了这个衰减率和适当的约束条件，数值就会收敛。

.. admonition:: 你应该知道

    尽管这两种收益的公式看起来差距很明显，但在深度强化学习中我们经常会混用。比如说，我们经常会用算法优化无折扣的收益，但是用衰减率估算 **值函数**。    

.. _`不收敛`: https://en.wikipedia.org/wiki/Convergent_series

强化学习问题
--------------

**expected return** when the agent acts according to it.
无论选择哪种方式衡量收益（有限无折扣或无限折扣），无论选择哪种策略，强化学习的目标都是选择一种策略获得智能体的最大化 **预期收益**。

讨论预期收益之前，我们先探探行动轨迹的可能性分布。

我们假设环境转换和策略都是随机的。这种情况下， :math:`T` -步 行动轨迹是：

.. math::

    P(\tau|\pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t).

预期收益是 :math:`J(\pi)`

.. math::

    J(\pi) = \int_{\tau} P(\tau|\pi) R(\tau) = \underE{\tau\sim \pi}{R(\tau)}.

强化学习中的核心优化问题可以表示为：

.. math::

    \pi^* = \arg \max_{\pi} J(\pi),

 :math:`\pi^*` 是 **最优策略**

值函数
---------------

知道一个状态的值或者状态行动对(state-action pair)很有用。这里的值指的是，如果你从某一个状态或者状态行动对开始，一直按照某个策略运行下去最终获得的期望回报。 **值函数** 几乎被用在每一个强化学习的方法中。

这里介绍四种函数：

1.同策略值函数： :math:`V^{\pi}(s)`，从某一个状态 :math:`s` 开始，之后每一步都按照策略 :math:`\pi` 执行
    .. math::
        
        V^{\pi}(s) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}

2.同策略行动-值函数： :math:`Q^{\pi}(s,a)`,从某一个状态 :math:`s` 开始，先随便执行一个行动 :math:`a` （有可能不是按照策略走的），之后每一步都按照固定的策略执行 :math:`\pi`

    .. math::
        
        Q^{\pi}(s,a) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}


3. 最优值函数： :math:`V^*(s)`，从某一个状态 :math:`s` 开始，之后每一步走执行 *最优策略* 策略 :math:`\pi`

    .. math::

        V^*(s) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}

4.  最优行动-值函数： :math:`Q^*(s,a)` ，从某一个状态 :math:`s` 开始，先随便执行一个行动 :math:`a` （有可能不是按照策略走的），之后每一步都按照 *最优策略* 执行 :math:`\pi`

    .. math::

        Q^*(s,a) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}

.. admonition:: 你应该知道

    当我们讨论值函数的时候，如果我们没有特指依赖时间，那就意味着 **无限折扣收益**。 有限无折扣收益需要传入时间作为参数，你知道为什么吗？ 提示：时间到了会发生什么？

.. admonition:: 你应该知道

    值函数和行动-值函数两者之间经常出现的联系：

    .. math::

        V^{\pi}(s) = \underE{a\sim \pi}{Q^{\pi}(s,a)},

    以及：

    .. math::

        V^*(s) = \max_a Q^* (s,a).

    这些关系直接来自刚刚给出的定义，你能尝试证明吗？

最优Q函数和最优行动
---------------------------------------------

最优行动-值函数 :math:`Q^*(s,a)` 和被最优策略选中的行动有重要的联系。从定义上讲， :math:`Q^*(s,a)` 指的是从一个状态 :math:`s`开始，任意执行一个行动 :math:`a`，然后一直按照最优策略执行下去所获得的回报。 

最优策略 :math:`s`会选择从状态 :math:`s`开始选择能够最大化期望回报的行动。所以如果我们有了 :math:`Q^*`，就可以通过下面的公式直接获得最优行动： :math:`a^*(s)`：

.. math::

    a^*(s) = \arg \max_a Q^* (s,a).

注意：可能会有多个行为能够最大化 :math:`Q^*(s,a)`，这种情况下，它们都是最优行为，最优策略可能会从中随机选择一个。但是总会存在一个最优策略每一步选择行为的时候是确定的。

贝尔曼方程
-----------------

全部四个值函数都遵守自一致性的方程叫做 **贝尔曼方程**，贝尔曼方程的基本思想是：

    起始点的值等于当前点预期值和下一个点的值之和。
    
通策略值函数的贝尔曼方程：

.. math::
    :nowrap:

    \begin{align*}
    V^{\pi}(s) &= \underE{a \sim \pi \\ s'\sim P}{r(s,a) + \gamma V^{\pi}(s')}, \\
    Q^{\pi}(s,a) &= \underE{s'\sim P}{r(s,a) + \gamma \underE{a'\sim \pi}{Q^{\pi}(s',a')}},
    \end{align*}

:math:`s' \sim P` 是 :math:`s' \sim P(\cdot |s,a)` 的简写, 表明下一个状态 :math:`s'` 是按照转换规则从环境中抽样得到的; :math:`a \sim \pi` 是 :math:`a \sim \pi(\cdot|s)` 的简写; and :math:`a' \sim \pi`是 :math:`a' \sim \pi(\cdot|s')` 的简写. 

最优值函数的贝尔曼方程是：

.. math::
    :nowrap:

    \begin{align*}
    V^*(s) &= \max_a \underE{s'\sim P}{r(s,a) + \gamma V^*(s')}, \\
    Q^*(s,a) &= \underE{s'\sim P}{r(s,a) + \gamma \max_{a'} Q^*(s',a')}.
    \end{align*}

同策略值函数和最优值函数的贝尔曼方程最大的区别是有无 :math:`\max` 。这表明智能体在选择下一步行动时，为了做出最优行动，他必须选择能获得最大值的行动。

.. admonition:: 你应该知道

    贝尔曼算子（Bellman backup）会在强化学习中经常出现。对于一个状态或一个状态行动对，贝尔曼算子是贝尔曼方程的右边： 奖励加上一个值。
    
优势函数（Advantage Functions）
-------------------

强化学习中，有些时候我们不需要描述一个行动的绝对好坏，而只需要知道它相对于平均水平的优势。也就是说，我们只想知道一个行动的相对 **优势** 。这个概念就被称为优势函数。

一个服从策略 :math:`\pi`的优势函数，描述的是它在状态 :math:`s`下采取行为 :math:`a`比随机选择一个行为好多少（假设之后一直服从策略 :math:`\pi` ）。数学角度上，优势函数的定义为：

.. math::

    A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s).

.. admonition:: 你应该知道

    我们之后会继续谈论这部分，优势函数对于策略梯度方法来说非常重要。

数学模型（可选）
====================

我们已经非正式地讨论了智能体的环境，但是如果你深入研究，可能会发现这样的标准数学形式：**马尔科夫决策过程** (Markov Decision Processes, MDPs).MDP是一个5元组 :math:`\langle S, A, R, P, \rho_0 \rangle`, 其中

* :math:`S` 是所有有效状态的集合,
* :math:`A` 是所有有效动作的集合,
* :math:`R : S \times A \times S \to \mathbb{R}` 是奖励函数，其中 :math:`r_t = R(s_t, a_t, s_{t+1})`,
* :math:`P : S \times A \to \mathcal{P}(S)` 是转态转移的规则，其中 :math:`P(s'|s,a)` 是在状态  :math:`s` 下 采取动作 :math:`a` 转移到状态 :math:`s'` 的概率。 
* and :math:`\rho_0` is the starting state distribution.

.. _`Markov property`: https://en.wikipedia.org/wiki/Markov_property

