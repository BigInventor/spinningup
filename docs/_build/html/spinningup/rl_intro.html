

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>第一部分：强化学习中的核心概念 &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="第二部分：各种各种的强化学习算法" href="rl_intro2.html" />
    <link rel="prev" title="绘制结果" href="../user/plotting.html" /> 
 <script type="text/javascript">
 
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?7e494634f392b55baa85cfd2b508ae23";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

 
 </script> 


  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">用户文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">运行试验</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">试验输出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">绘制结果</a></li>
</ul>
<p class="caption"><span class="caption-text">强化学习介绍</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">第一部分：强化学习中的核心概念</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id3">强化学习能做什么</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id6">核心概念和术语</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id7">状态和观察</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">动作空间</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">策略</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id10">确定性策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">随机性策略</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id14">运动轨迹</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id15">奖励和回报</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id17">强化学习问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id18">值函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#q">最优Q函数和最优行动</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id19">贝尔曼方程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advantage-functions">优势函数（Advantage Functions）</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id20">数学模型（可选）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro2.html">第二部分：各种各种的强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro3.html">第三部分：策略优化介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">资源</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="spinningup.html">深度强化学习研究者的资料</a></li>
<li class="toctree-l1"><a class="reference internal" href="keypapers.html">深度强化学习的核心论文</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercises.html">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="bench.html">Benchmarks for Spinning Up Implementations</a></li>
</ul>
<p class="caption"><span class="caption-text">算法文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">工具文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">日志打印</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">绘图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI 工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">运行工具</a></li>
</ul>
<p class="caption"><span class="caption-text">其他</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">作者</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/translator.html">关于译者</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>第一部分：强化学习中的核心概念</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/spinningup/rl_intro.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1><a class="toc-backref" href="#id23">第一部分：强化学习中的核心概念</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="id2">
<p class="topic-title first">目录</p>
<ul class="simple">
<li><a class="reference internal" href="#id1" id="id23">第一部分：强化学习中的核心概念</a><ul>
<li><a class="reference internal" href="#id3" id="id24">强化学习能做什么</a></li>
<li><a class="reference internal" href="#id6" id="id25">核心概念和术语</a></li>
<li><a class="reference internal" href="#id20" id="id26">数学模型（可选）</a></li>
</ul>
</li>
</ul>
</div>
<p>欢迎来到强化学习的介绍部分！我们希望你能了解以下内容：</p>
<ul class="simple">
<li>常见的符号表示</li>
<li>高层次的理解：关于强化学习算法做什么（我们会尽量避免 <em>如何做</em> 这个话题）</li>
<li>算法背后的核心数学知识</li>
</ul>
<p>总的来说，强化学习是关于智能体以及它们如何通过试错来学习的研究。它确定了通过奖励或惩罚智能体的动作从而使它未来更容易重复或者放弃某一动作的思想。</p>
<div class="section" id="id3">
<h2><a class="toc-backref" href="#id24">强化学习能做什么</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>基于强化学习的方法已经在很多地方取得了成功。例如，它被用来教电脑在仿真环境下控制机器人：</p>
<video autoplay="" src="https://storage.googleapis.com/joschu-public/knocked-over-stand-up.mp4" loop="" controls="" style="display: block; margin-left: auto; margin-right: auto; margin-bottom:1.5em; width: 100%; max-width: 720px; max-height: 80vh;">
</video><p>以及在现实世界中的机器人：</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="https://www.youtube.com/embed/jwSbzNHGflM?ecver=1" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div>
<br /><p>强化学习因为被用在复杂策略游戏创造出突破性的 AI 中而名声大噪，最著名的要数 <a class="reference external" href="https://deepmind.com/research/alphago/">围棋</a> 、 <a class="reference external" href="https://blog.openai.com/openai-five/">Dota</a> 、教电脑 <a class="reference external" href="https://deepmind.com/research/dqn/">玩Atari游戏</a> 以及训练模拟机器人 <a class="reference external" href="https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/">听从人类的指令</a> 。</p>
</div>
<div class="section" id="id6">
<h2><a class="toc-backref" href="#id25">核心概念和术语</a><a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center" id="id22">
<img alt="../_images/rl_diagram_transparent_bg.png" src="../_images/rl_diagram_transparent_bg.png" />
<p class="caption"><span class="caption-text">智能体和环境的循环作用</span></p>
</div>
<p>强化学习的主要角色是 <strong>智能体</strong> 和 <strong>环境</strong>,环境是智能体存在和互动的世界。智能体在每一步的交互中，都会获得对于所处环境状态的观察（有可能只是一部分），然后决定下一步要执行的动作。环境会因为智能体对它的动作而改变，也可能自己改变。</p>
<p>智能体也会从环境中感知到 <strong>奖励</strong> 信号，一个表明当前状态好坏的数字。智能体的目标是最大化累计奖励，也就是 <strong>回报</strong>。强化学习就是智能体通过学习来完成目标的方法。</p>
<p>为了便于后面的学习，我们介绍一些术语：</p>
<ul class="simple">
<li>状态和观察(states and observations)</li>
<li>动作空间(action spaces)</li>
<li>策略(policies)</li>
<li>行动轨迹(trajectories)</li>
<li>不同的回报公式(formulations of return)</li>
<li>强化学习优化问题(the RL optimization problem)</li>
<li>值函数(value functions)</li>
</ul>
<div class="section" id="id7">
<h3>状态和观察<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>一个 <strong>状态</strong> <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 是一个关于这个世界状态的完整描述。这个世界除了状态以外没有别的信息。<strong>观察</strong> <img class="math" src="../_images/math/a97d2a90fb666358380ca3bbc433d8f9ab7c7e42.svg" alt="o"/> 是对于一个状态的部分描述，可能会漏掉一些信息。</p>
<p>在深度强化学习中，我们一般用 <a class="reference external" href="https://en.wikipedia.org/wiki/Real_coordinate_space">实数向量、矩阵或者更高阶的张量（tensor）</a> 表示状态和观察。比如说，视觉上的 <strong>观察</strong> 可以用RGB矩阵的方式表示其像素值；机器人的 <strong>状态</strong> 可以通过关节角度和速度来表示。</p>
<p>如果智能体观察到环境的全部状态，我们通常说环境是被 <strong>全面观察</strong> 的。如果智能体只能观察到一部分，我们称之为 <strong>部分观察</strong>。</p>
<div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p>强化学习有时候用这个符号 <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 代表状态 , 有些地方也会写作观察符号 <img class="math" src="../_images/math/a97d2a90fb666358380ca3bbc433d8f9ab7c7e42.svg" alt="o"/>.  尤其是，当智能体在决定采取什么动作的时候，符号上的表示按理动作是基于状态的，但实际上，动作是基于观察的，因为智能体并不能知道状态（只能通过观察了解状态）。</p>
<p class="last">在我们的教程中，我们会按照标准的方式使用这些符号，不过你一般能从上下文中看出来具体表示什么。如果你觉得有些内容不够清楚，请提出issue！我们的目的是教会大家，不是让大家混淆。</p>
</div>
</div>
<div class="section" id="id8">
<h3>动作空间<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>不同的环境有不同的动作。所有有效动作的集合称之为 <strong>动作空间</strong>。有些环境，比如说 Atari 游戏和围棋，属于 <strong>离散动作空间</strong>，这种情况下智能体只能采取有限的动作。其他的一些环境，比如智能体在物理世界中控制机器人，属于 <strong>连续动作空间</strong>。在连续动作空间中，动作是实数向量。</p>
<p>这种区别对于深度强化学习来说，影响深远。有些种类的算法只能直接用在某些案例上，如果需要用在别的地方，可能就需要大量重写代码。</p>
</div>
<div class="section" id="id9">
<h3>策略<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p><strong>策略</strong> 是智能体用于决定下一步执行什么行动的规则。可以是确定性的，一般表示为：<img class="math" src="../_images/math/a6942f7ce9211a8f804acefc4aa92917669b10e7.svg" alt="\mu"/>:</p>
<div class="math">
<p><img src="../_images/math/ba295800a1bb298f8754e7d2c217df79733f03b1.svg" alt="a_t = \mu(s_t),"/></p>
</div><p>也可以是随机的，一般表示为 <img class="math" src="../_images/math/6abd2b6bd2a10c499ace309d93cbfc9a48fbb708.svg" alt="\pi"/>:</p>
<div class="math">
<p><img src="../_images/math/e06a43325cfc5887883b2d2fedea26961408c09b.svg" alt="a_t \sim \pi(\cdot | s_t)."/></p>
</div><p>因为策略本质上就是智能体的大脑，所以很多时候“策略”和“智能体”这两个名词经常互换，例如我们会说：“策略的目的是最大化奖励”。</p>
<p>在深度强化学习中，我们处理的是参数化的策略，这些策略的输出，依赖于一系列计算函数，而这些函数又依赖于参数（例如神经网络的权重和误差），所以我们可以通过一些优化算法改变智能体的的行为。</p>
<p>我们经常把这些策略的参数写作 <img class="math" src="../_images/math/a9bf14f8a79463d05190aceda63fa75b2c001bfe.svg" alt="\theta"/> 或者 <img class="math" src="../_images/math/466a6be907a4e972b59aa089dcf9dc57df77e331.svg" alt="\phi"/> ，然后把它写在策略的下标上来强调两者的联系。</p>
<div class="math">
<p><img src="../_images/math/7e17f721097166fb416c9a54fcce13475a88d7e4.svg" alt="a_t &amp;= \mu_{\theta}(s_t) \\
a_t &amp;\sim \pi_{\theta}(\cdot | s_t)."/></p>
</div><div class="section" id="id10">
<h4>确定性策略<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h4>
<p><strong>例子：确定性策略：</strong> 这是一个基于 TensorFlow 在连续动作空间上确定性策略的简单例子：</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">obs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">obs_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
<span class="n">actions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">act_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</div>
<p>其中，<em>mlp</em> 是把多个给定大小和激活函数的 <em>密集层</em> （dense layer）相互堆积在一起的函数。</p>
</div>
<div class="section" id="id11">
<h4>随机性策略<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h4>
<p>深度强化学习中最常见的两种随机策略是 <strong>绝对策略**(Categorical Policies) 和 **对角高斯策略</strong> (Diagonal Gaussian Policies)。</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">确定</a> 策略适用于离散行动空间，而
<a class="reference external" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">高斯</a> 策略一般用在连续行动空间</p>
<p>使用和训练随机策略的时候有两个重要的计算：</p>
<ul class="simple">
<li>从策略中采样行动</li>
<li>计算特定行为的似然(likelihoods) <img class="math" src="../_images/math/4f3dbd46b18f2368c446a2494897eaaa9672c34f.svg" alt="\log \pi_{\theta}(a|s)"/>.</li>
</ul>
<p>下面我们介绍一下这两种策略</p>
<div class="admonition- admonition">
<p class="first admonition-title">绝对策略</p>
<p>确定策略就像是一个离散空间的分类器(classifier)。对于分类器和确定策略来说，建立神经网络的方式一模一样：输入是观察，接着是一些卷积、全连接层之类的，至于具体是哪些取决于输入的类型，最后一个线性层给出每个行动的 log 数值(logits)，后面跟一个 <a class="reference external" href="https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax">softmax</a> 层把 log 数值转换为可能性。</p>
<p><strong>采样</strong> 给定每个行动的可能性，TensorFlow之类的框架有内置采样工。具体可查阅 <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distributions/Categorical">tf.distributions.Categorical</a>  或 <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/multinomial">tf.multinomial</a> 的文档。</p>
<p><strong>对数似然</strong> ：表示最后一层的可能性 <img class="math" src="../_images/math/3647cb0f679f1c35841ead61808305dd3f3de390.svg" alt="P_{\theta}(s)"/>。它是一个有很多值的向量，我们可以把行动当做向量的索引。所以向量的对数似然值 <img class="math" src="../_images/math/7299c243b08052a2a26e53de560e7002cb31b38f.svg" alt="a"/> 可以通过这样得到：</p>
<div class="last math">
<p><img src="../_images/math/cb6df623ba06b4b333b28d3289f5ed60b4d1ddce.svg" alt="\log \pi_{\theta}(a|s) = \log \left[P_{\theta}(s)\right]_a."/></p>
</div></div>
<div class="admonition- admonition">
<p class="first admonition-title">对角高斯策略</p>
<p>多元高斯分布（或者多元正态分布），可以用一个向量 <img class="math" src="../_images/math/a6942f7ce9211a8f804acefc4aa92917669b10e7.svg" alt="\mu"/> 和协方差 <img class="math" src="../_images/math/71a165c3b52dd48d4ebd501e6ff8b42e879d8bd6.svg" alt="\Sigma"/> 来描述。对角高斯分布就是协方差矩阵只有对角线上有值的特殊情况，所以我们可以用一个向量来表示它。</p>
<p>对角高斯策略总会有一个神经网络，表示观察到行动的映射。其中有两种协方差矩阵的经典表示方式：</p>
<p><strong>第一种</strong> ： 有一个单独的关于对数标准差的向量： <img class="math" src="../_images/math/6094d7f498eebb7e74ae14ec3091d5306c3d495d.svg" alt="\log \sigma"/>，它不是关于状态的函数，<img class="math" src="../_images/math/6094d7f498eebb7e74ae14ec3091d5306c3d495d.svg" alt="\log \sigma"/> 而是单独的参数（我们这个项目里，VPG, TRPO 和 PPO 都是用这种方式实现的）。</p>
<p><strong>第二种</strong> ：有一个神经网络，从状态映射到对数标准差 <img class="math" src="../_images/math/ef606981846509bf86ab51b58d1f41fb48a13a12.svg" alt="\log \sigma_{\theta}(s)"/>。这种方式可能会均值网络共享某些层的参数。</p>
<p>要注意这两种情况下我们都没有直接计算标准差而是对数标准差。这是因为对数标准差能够接受 <img class="math" src="../_images/math/fb6a4797ec953dfa299724fcd80ed1832807a829.svg" alt="(-\infty, \infty)"/> 的任何值，而标准差必须要求参数非负。要知道，限制条件越少，训练就越简单。而标准差可以通过取幂快速从对数标准差中计算得到，所以这种表示方法也不会丢失信息。</p>
<p><strong>采样</strong> ：给定平均行动  <img class="math" src="../_images/math/b28c83e6385c15489a3c64ad16ca263c6694b9eb.svg" alt="\mu_{\theta}(s)"/> 和 标准差 <img class="math" src="../_images/math/e00e5f8ff60f61f29a5a6a645f9185482691c4b9.svg" alt="\sigma_{\theta}(s)"/>，以及一个服从球形高斯分布的噪声向量 <img class="math" src="../_images/math/3b0deb3082207779ea190a2ae25e9aa279824b17.svg" alt="z"/>，行为的样本可以这样计算：</p>
<div class="math">
<p><img src="../_images/math/f8566f0945383c46b69d0b1329e0533f18400a18.svg" alt="a = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot z,"/></p>
</div><p>这里 <img class="math" src="../_images/math/173dc1e6ce20a6bfc2ca70cd4af687a9ade5bc0f.svg" alt="\odot"/> 表示两个向量按元素乘。标准框架都有内置噪声向量实现，例如  <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/random_normal">tf.random_normal</a> 。你也可以直接用 <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distributions/Normal">tf.distributions.Normal</a> 以均值和标准差的方式采样。</p>
<p><strong>对数似然</strong> 一个 k 维行动 <img class="math" src="../_images/math/7299c243b08052a2a26e53de560e7002cb31b38f.svg" alt="a"/> 基于均值为 <img class="math" src="../_images/math/4db93eb93a957bb2c87d42f5011173884fa427f3.svg" alt="\mu = \mu_{\theta}(s)"/>，标准差为 <img class="math" src="../_images/math/22b37daf3d9c20eda213c3734b90c9ab195a4098.svg" alt="\sigma = \sigma_{\theta}(s)"/> 的对角高斯的对数似然：</p>
<div class="last math">
<p><img src="../_images/math/fc279eedd68ed90829565a668b804d43593a3631.svg" alt="\log \pi_{\theta}(a|s) = -\frac{1}{2}\left(\sum_{i=1}^k \left(\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2 \log \sigma_i \right) + k \log 2\pi \right)."/></p>
</div></div>
</div>
</div>
<div class="section" id="id14">
<h3>运动轨迹<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>运动轨迹 <img class="math" src="../_images/math/bbe261810b01ddea8c6d6af3291c4f0b603daee4.svg" alt="\tau"/> 指的是状态和行动的序列。</p>
<div class="math">
<p><img src="../_images/math/c7b0ae3ae1ed076d02fe776d20cf5cf801432684.svg" alt="\tau = (s_0, a_0, s_1, a_1, ...)."/></p>
</div><p>第一个状态 <img class="math" src="../_images/math/b1c45c89f992115b577fce8e7faa1ec739e6297a.svg" alt="s_0"/>，是从 <strong>开始状态分布</strong> 中随机采样的，有时候表示为 <img class="math" src="../_images/math/3607328515af96dafab2766b882383d20739b1c0.svg" alt="\rho_0"/> :</p>
<div class="math">
<p><img src="../_images/math/3299561b349142dff365f3f8d2539e766f6d1cd0.svg" alt="s_0 \sim \rho_0(\cdot)."/></p>
</div><p>转态转换（从某一状态时间 <img class="math" src="../_images/math/2c6856336580e5ff7084f044fa6439e3fbf57fbf.svg" alt="t"/> , <img class="math" src="../_images/math/c59b1a38d2b4e73e2cf513f8af52474024ec9950.svg" alt="s_t"/> 到另一状态时间 <img class="math" src="../_images/math/a51e798b6bc5db6b9ab213bc29a1fdeff473ad19.svg" alt="t+1"/> , <img class="math" src="../_images/math/92113d0c315b5197bf9547e7a6dbdcce2b695aca.svg" alt="s_{t+1}"/> 会发生什么），是由环境的自然法则确定的，并且只依赖于最近的行动 <img class="math" src="../_images/math/3dfb5136b238206b55a17f55c71c4921a3536fb5.svg" alt="a_t"/>。它们可以是确定性的：</p>
<div class="math">
<p><img src="../_images/math/a652bd39553ae4010ff527b5007ebc495019804e.svg" alt="s_{t+1} = f(s_t, a_t)"/></p>
</div><p>而可以是随机的：</p>
<div class="math">
<p><img src="../_images/math/428731da06a860db6884c597e877bc7b940afc72.svg" alt="s_{t+1} \sim P(\cdot|s_t, a_t)."/></p>
</div><p>智能体的行为由策略确定。</p>
<div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p class="last">行动轨迹常常也被称作 <strong>episodes</strong> 或者 <strong>rollouts</strong>。</p>
</div>
</div>
<div class="section" id="id15">
<h3>奖励和回报<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>强化学习中，奖励函数 <img class="math" src="../_images/math/d4e98f849929420fa6fe245998b7764f7f0a3b3e.svg" alt="R"/> 非常重要。它由当前状态、已经执行的行动和下一步的状态共同决定。</p>
<div class="math">
<p><img src="../_images/math/2393b6354c85e8bb305133fd08b62084ed3a64ff.svg" alt="r_t = R(s_t, a_t, s_{t+1})"/></p>
</div><p>有时候这个公式会被改成只依赖当前的状态 <img class="math" src="../_images/math/d4fe03dd0163385f0844c31b77488f435b59fac8.svg" alt="r_t = R(s_t)"/>，或者状态行动对 <img class="math" src="../_images/math/db2f78cfcd28e97aac7e951b96e6b23759284728.svg" alt="r_t = R(s_t,a_t)"/>。</p>
<p>智能体的目标是最大化行动轨迹的累计奖励，这意味着很多事情。我们会把所有的情况表示为 <img class="math" src="../_images/math/a4cfe11c3e8bb64a3fbcadd1e90e64d7488ab724.svg" alt="R(\tau)"/>，至于具体表示什么，要么可以很清楚的从上下文看出来，要么并不重要。（因为相同的方程式适用于所有情况。）</p>
<p><strong>无衰减收益</strong>，指的是在一个固定窗口步数内获得的累计奖励：</p>
<div class="math">
<p><img src="../_images/math/7930a61633ed77668804165f84dfb986e97c1c0a.svg" alt="R(\tau) = \sum_{t=0}^T r_t."/></p>
</div><p>另一种叫做 <strong>衰减收益</strong>，指的是智能体获得的全部奖励之和，但是奖励会因为获得的时间不同而衰减。这个公式包含衰减率 <img class="math" src="../_images/math/47cd85a3b6e025ebade63d4c2e89c6a6e8328ea8.svg" alt="\gamma \in (0,1)"/>:</p>
<div class="math">
<p><img src="../_images/math/a0f534d5571d04c1c0d00f036b9b803b0e95e5f3.svg" alt="R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t."/></p>
</div><p>这里为什么要加上一个衰减率呢？为什么不直接把所有的奖励加在一起？可以从两个角度来解释： 直观上讲，现在的奖励比外来的奖励要好，所以未来的奖励会衰减；数学角度上，无限多个奖励的和很可能 <a class="reference external" href="https://en.wikipedia.org/wiki/Convergent_series">不收敛</a> ，有了衰减率和适当的约束条件，数值才会收敛。</p>
<div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p class="last">这两个公式看起来差距很大，事实上我们经常会混用。比如说，我们经常会用算法优化无衰减的回报，但是用衰减率估算 <strong>值函数</strong>。</p>
</div>
</div>
<div class="section" id="id17">
<h3>强化学习问题<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>无论选择哪种方式衡量收益（无衰减收益或衰减收益），无论选择哪种策略，强化学习的目标都是选择一种策略从而最大化 <strong>预期收益</strong>。</p>
<p>讨论预期收益之前，我们先讨论下行动轨迹的可能性分布。</p>
<p>我们假设环境转换和策略都是随机的。这种情况下， <img class="math" src="../_images/math/c3a09339f0c7f42a4ba2b3163f5f3184319b9f4c.svg" alt="T"/> 步 行动轨迹是：</p>
<div class="math">
<p><img src="../_images/math/0ce9798fbfa313b4eec8a4bc8b0f16eeadf66ac2.svg" alt="P(\tau|\pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t)."/></p>
</div><p>预期收益是 <img class="math" src="../_images/math/f28e5a076d679a1980b0d133b56b70cf8ae2fdc1.svg" alt="J(\pi)"/></p>
<div class="math">
<p><img src="../_images/math/5a94a331f6e28613a5037981cf00f46f39e4ca1d.svg" alt="J(\pi) = \int_{\tau} P(\tau|\pi) R(\tau) = \underE{\tau\sim \pi}{R(\tau)}."/></p>
</div><p>强化学习中的核心优化问题可以表示为：</p>
<div class="math">
<p><img src="../_images/math/ed3027c01213c453a4e580898a919e9beb501c3b.svg" alt="\pi^* = \arg \max_{\pi} J(\pi),"/></p>
</div><p><img class="math" src="../_images/math/be64fb689087a153fbabb0b62fb14bbe7f4f22fb.svg" alt="\pi^*"/> 是 <strong>最优策略</strong></p>
</div>
<div class="section" id="id18">
<h3>值函数<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>知道一个状态的 <strong>值</strong> 或者状态行动对(state-action pair)很有用。这里的值指的是，如果你从某一个状态或者状态行动对开始，一直按照某个策略运行下去最终获得的期望回报。几乎是所有的强化学习方法，都在用不同的形式使用着值函数。</p>
<p>这里介绍四种主要函数：</p>
<ol class="arabic">
<li><dl class="first docutils">
<dt><strong>同策略值函数</strong> ： <img class="math" src="../_images/math/e8cac649e08a2b01c72c546971fe8a2bd817075a.svg" alt="V^{\pi}(s)"/>，从某一个状态 <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 开始，之后每一步行动都按照策略 <img class="math" src="../_images/math/6abd2b6bd2a10c499ace309d93cbfc9a48fbb708.svg" alt="\pi"/> 执行</dt>
<dd><div class="first last math">
<p><img src="../_images/math/24e85556d8e8b46054eeefaa333718b4217b9307.svg" alt="V^{\pi}(s) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}"/></p>
</div></dd>
</dl>
</li>
<li><p class="first"><strong>同策略行动-值函数</strong> ： <img class="math" src="../_images/math/3f0d7103dfb3c994f9d865a6dba65274a3600260.svg" alt="Q^{\pi}(s,a)"/>,从某一个状态 <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 开始，先随便执行一个行动 <img class="math" src="../_images/math/7299c243b08052a2a26e53de560e7002cb31b38f.svg" alt="a"/> （有可能不是按照策略走的），之后每一步都按照固定的策略执行 <img class="math" src="../_images/math/6abd2b6bd2a10c499ace309d93cbfc9a48fbb708.svg" alt="\pi"/></p>
<blockquote>
<div><div class="math">
<p><img src="../_images/math/806936eb5be33dcd2e6c9608c10f2007a6830794.svg" alt="Q^{\pi}(s,a) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}"/></p>
</div></div></blockquote>
</li>
<li><p class="first"><strong>最优值函数</strong>： <img class="math" src="../_images/math/28754060ab6e94a9f0f0daac3f8a20d6b75cf829.svg" alt="V^*(s)"/>，从某一个状态 <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 开始，之后每一步都按照 <em>最优策略</em>  <img class="math" src="../_images/math/6abd2b6bd2a10c499ace309d93cbfc9a48fbb708.svg" alt="\pi"/> 执行</p>
<blockquote>
<div><div class="math">
<p><img src="../_images/math/04891dc63aa7cf0e2cec6dc33ec0e3a173163638.svg" alt="V^*(s) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}"/></p>
</div></div></blockquote>
</li>
<li><p class="first"><strong>最优行动-值函数</strong> ： <img class="math" src="../_images/math/29face1cf6ea248f89bd640b6aafee497ba10a94.svg" alt="Q^*(s,a)"/> ，从某一个状态 <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 开始，先随便执行一个行动 <img class="math" src="../_images/math/7299c243b08052a2a26e53de560e7002cb31b38f.svg" alt="a"/> （有可能不是按照策略走的），之后每一步都按照 <em>最优策略</em> 执行 <img class="math" src="../_images/math/6abd2b6bd2a10c499ace309d93cbfc9a48fbb708.svg" alt="\pi"/></p>
<div class="math">
<p><img src="../_images/math/854cac0d0d38f2666d9a9b595f485095f4705667.svg" alt="Q^*(s,a) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}"/></p>
</div></li>
</ol>
<div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p class="last">当我们讨论值函数的时候，如果我们没有提到时间依赖问题，那就意味着 <strong>折扣收益</strong>。 无衰减收益需要传入时间作为参数，你知道为什么吗？ 提示：时间到了会发生什么？</p>
</div>
<div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p>值函数和行动-值函数两者之间经常出现的联系：</p>
<div class="math">
<p><img src="../_images/math/29b97095f79e6c186aa2e3e758c1487b7864b982.svg" alt="V^{\pi}(s) = \underE{a\sim \pi}{Q^{\pi}(s,a)},"/></p>
</div><p>以及：</p>
<div class="math">
<p><img src="../_images/math/af8a6558740b306546e626624653dbb52f7f1325.svg" alt="V^*(s) = \max_a Q^* (s,a)."/></p>
</div><p class="last">这些关系直接来自刚刚给出的定义，你能尝试证明吗？</p>
</div>
</div>
<div class="section" id="q">
<h3>最优Q函数和最优行动<a class="headerlink" href="#q" title="Permalink to this headline">¶</a></h3>
<p>最优行动-值函数 <img class="math" src="../_images/math/29face1cf6ea248f89bd640b6aafee497ba10a94.svg" alt="Q^*(s,a)"/> 和被最优策略选中的行动有重要的联系。从定义上讲， <img class="math" src="../_images/math/29face1cf6ea248f89bd640b6aafee497ba10a94.svg" alt="Q^*(s,a)"/> 指的是从一个状态 <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 开始，任意执行一个行动 <img class="math" src="../_images/math/7299c243b08052a2a26e53de560e7002cb31b38f.svg" alt="a"/> ，然后一直按照最优策略执行下去所获得的回报。</p>
<p>最优策略 <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 会选择从状态 <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 开始选择能够最大化期望回报的行动。所以如果我们有了 <img class="math" src="../_images/math/d62537306703802cdc91716cba6a3728dc03c06e.svg" alt="Q^*"/> ，就可以通过下面的公式直接获得最优行动： <img class="math" src="../_images/math/4b956566dafb7b81bb0af17e93b71f160bfccacc.svg" alt="a^*(s)"/> ：</p>
<div class="math">
<p><img src="../_images/math/c6ff7555657ab0f5b8161943dc2bd10a4d3409f7.svg" alt="a^*(s) = \arg \max_a Q^* (s,a)."/></p>
</div><p>注意：可能会有多个行为能够最大化 <img class="math" src="../_images/math/29face1cf6ea248f89bd640b6aafee497ba10a94.svg" alt="Q^*(s,a)"/>，这种情况下，它们都是最优行为，最优策略可能会从中随机选择一个。但是总会存在一个最优策略每一步选择行为的时候是确定的。</p>
</div>
<div class="section" id="id19">
<h3>贝尔曼方程<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<p>全部四个值函数都遵守自一致性的方程叫做 <strong>贝尔曼方程</strong>，贝尔曼方程的基本思想是：</p>
<blockquote>
<div>起始点的值等于当前点预期值和下一个点的值之和。</div></blockquote>
<p>同策略值函数的贝尔曼方程：</p>
<div class="math">
<p><img src="../_images/math/6759a843554080ee5ceea8964241a568a61b17cc.svg" alt="\begin{align*}
V^{\pi}(s) &amp;= \underE{a \sim \pi \\ s'\sim P}{r(s,a) + \gamma V^{\pi}(s')}, \\
Q^{\pi}(s,a) &amp;= \underE{s'\sim P}{r(s,a) + \gamma \underE{a'\sim \pi}{Q^{\pi}(s',a')}},
\end{align*}"/></p>
</div><p><img class="math" src="../_images/math/a3b29f8d11204158eb342a065ddbea1e53b3f392.svg" alt="s' \sim P"/> 是 <img class="math" src="../_images/math/66175a763ae516a3e95f0f617e4253f55923d553.svg" alt="s' \sim P(\cdot |s,a)"/> 的简写, 表明下一个状态 <img class="math" src="../_images/math/2767335e46fe0770449b884e214f8b3df8958031.svg" alt="s'"/> 是按照转换规则从环境中抽样得到的; <img class="math" src="../_images/math/6b35443d82011dd27056988fd8970baa5e1f5074.svg" alt="a \sim \pi"/> 是 <img class="math" src="../_images/math/26fc7605d39d44e08b69d89fc7c8819faaaaddbb.svg" alt="a \sim \pi(\cdot|s)"/> 的简写; and <img class="math" src="../_images/math/9927fd157092bd4a8789d2a19bfec8b5dc137bce.svg" alt="a' \sim \pi"/> 是 <img class="math" src="../_images/math/aa53e07436ccd4ac23e6902885b21720e7dc8f9e.svg" alt="a' \sim \pi(\cdot|s')"/> 的简写.</p>
<p>最优值函数的贝尔曼方程是：</p>
<div class="math">
<p><img src="../_images/math/ccee7495b30c8a60793d09b3645975ac69e14a08.svg" alt="\begin{align*}
V^*(s) &amp;= \max_a \underE{s'\sim P}{r(s,a) + \gamma V^*(s')}, \\
Q^*(s,a) &amp;= \underE{s'\sim P}{r(s,a) + \gamma \max_{a'} Q^*(s',a')}.
\end{align*}"/></p>
</div><p>同策略值函数和最优值函数的贝尔曼方程最大的区别是是否在行动中去 <img class="math" src="../_images/math/edd56bf7761ff39dfdc7ca13b2641af21accc5bd.svg" alt="\max"/> 。这表明智能体在选择下一步行动时，为了做出最优行动，他必须选择能获得最大值的行动。</p>
<div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p class="last">贝尔曼算子（Bellman backup）会在强化学习中经常出现。对于一个状态或一个状态行动对，贝尔曼算子是贝尔曼方程的右边： 奖励加上一个值。</p>
</div>
</div>
<div class="section" id="advantage-functions">
<h3>优势函数（Advantage Functions）<a class="headerlink" href="#advantage-functions" title="Permalink to this headline">¶</a></h3>
<p>强化学习中，有些时候我们不需要描述一个行动的绝对好坏，而只需要知道它相对于平均水平的优势。也就是说，我们只想知道一个行动的相对 <strong>优势</strong> 。这就是优势函数的概念。</p>
<p>一个服从策略 <img class="math" src="../_images/math/6abd2b6bd2a10c499ace309d93cbfc9a48fbb708.svg" alt="\pi"/> 的优势函数，描述的是它在状态 <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 下采取行为 <img class="math" src="../_images/math/7299c243b08052a2a26e53de560e7002cb31b38f.svg" alt="a"/> 比随机选择一个行为好多少（假设之后一直服从策略 <img class="math" src="../_images/math/6abd2b6bd2a10c499ace309d93cbfc9a48fbb708.svg" alt="\pi"/>  ）。数学角度上，优势函数的定义为：</p>
<div class="math">
<p><img src="../_images/math/dff81dfb82c7657f8e15754a1d6803a38c43e3b8.svg" alt="A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)."/></p>
</div><div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p class="last">我们之后会继续谈论优势函数，它对于策略梯度方法非常重要。</p>
</div>
</div>
</div>
<div class="section" id="id20">
<h2><a class="toc-backref" href="#id26">数学模型（可选）</a><a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h2>
<p>我们已经非正式地讨论了智能体的环境，但是如果你深入研究，可能会发现这样的标准数学形式：<strong>马尔科夫决策过程</strong> (Markov Decision Processes, MDPs)。MDP是一个5元组 <img class="math" src="../_images/math/8856e84a582f7587b47f6cc8c7846da6994492e9.svg" alt="\langle S, A, R, P, \rho_0 \rangle"/>，其中</p>
<ul class="simple">
<li><img class="math" src="../_images/math/54f67ffbc0534f8d941160590017216926db1975.svg" alt="S"/> 是所有有效状态的集合,</li>
<li><img class="math" src="../_images/math/e03dd1414bcec5b3baac929fbed8ba0ef00b2d0b.svg" alt="A"/> 是所有有效动作的集合,</li>
<li><img class="math" src="../_images/math/d424f8df005c1a370f1be27a7d7827895b36451c.svg" alt="R : S \times A \times S \to \mathbb{R}"/> 是奖励函数，其中 <img class="math" src="../_images/math/ecc7ba14305238ee709bb0e6bd888c500f7f6c5b.svg" alt="r_t = R(s_t, a_t, s_{t+1})"/>,</li>
<li><img class="math" src="../_images/math/32d7cfdd3132ef22cbabbf8c37729375368a755a.svg" alt="P : S \times A \to \mathcal{P}(S)"/> 是转态转移的规则，其中 <img class="math" src="../_images/math/7b7f50f2f22d884bf4bfaab439ad6f4e95d7de85.svg" alt="P(s'|s,a)"/> 是在状态  <img class="math" src="../_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg" alt="s"/> 下 采取动作 <img class="math" src="../_images/math/7299c243b08052a2a26e53de560e7002cb31b38f.svg" alt="a"/> 转移到状态 <img class="math" src="../_images/math/2767335e46fe0770449b884e214f8b3df8958031.svg" alt="s'"/> 的概率。</li>
<li><img class="math" src="../_images/math/3607328515af96dafab2766b882383d20739b1c0.svg" alt="\rho_0"/> 是开始状态的分布。</li>
</ul>
<p>马尔科夫决策过程指的是服从 <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_property">马尔科夫性</a> 的系统： 状态转移只依赖与最近的状态和行动，而不依赖之前的历史数据。</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rl_intro2.html" class="btn btn-neutral float-right" title="第二部分：各种各种的强化学习算法" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../user/plotting.html" class="btn btn-neutral" title="绘制结果" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright OpenAI提供教程，由曹真翻译。.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>