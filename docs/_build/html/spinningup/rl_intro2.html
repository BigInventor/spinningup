

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>第二部分：各种各种的强化学习算法 &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="第三部分：策略优化介绍" href="rl_intro3.html" />
    <link rel="prev" title="第一部分：强化学习中的核心概念" href="rl_intro.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">用户文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">运行试验</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">试验输出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">绘制结果</a></li>
</ul>
<p class="caption"><span class="caption-text">强化学习介绍</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="rl_intro.html">第一部分：强化学习中的核心概念</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">第二部分：各种各种的强化学习算法</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id3">强化学习算法的分类</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-free-vs-model-based">免模型学习（Model-Free） vs 有模型学习（Model-Based）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">要学习什么</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">免模型学习中要学习什么</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id11">有模型强化学习要学习什么</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id13">分类中提到的算法链接</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro3.html">第三部分：策略优化介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">资源</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="spinningup.html">Spinning Up as a Deep RL Researcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="keypapers.html">深度强化学习的核心论文</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercises.html">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="bench.html">Benchmarks for Spinning Up Implementations</a></li>
</ul>
<p class="caption"><span class="caption-text">算法文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">工具文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">日志打印</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">绘图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI 工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">运行工具</a></li>
</ul>
<p class="caption"><span class="caption-text">其他</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">作者</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/translator.html">关于译者</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>第二部分：各种各种的强化学习算法</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/spinningup/rl_intro2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1><a class="toc-backref" href="#id31">第二部分：各种各种的强化学习算法</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="id2">
<p class="topic-title first">目录</p>
<ul class="simple">
<li><a class="reference internal" href="#id1" id="id31">第二部分：各种各种的强化学习算法</a><ul>
<li><a class="reference internal" href="#id3" id="id32">强化学习算法的分类</a></li>
<li><a class="reference internal" href="#id13" id="id33">分类中提到的算法链接</a></li>
</ul>
</li>
</ul>
</div>
<p>我们已经介绍了深度学习的基础术语和符号，现在可以讨论更丰富的资料：现代强化学习的整体发展和算法设计时候要考虑的各种因素之间的权衡。</p>
<div class="section" id="id3">
<h2><a class="toc-backref" href="#id32">强化学习算法的分类</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center" id="id30">
<img alt="../_images/rl_algorithms_9_15.svg" src="../_images/rl_algorithms_9_15.svg" /><p class="caption"><span class="caption-text">一个不是很详细但是十分有用的现代强化学习算法分类。  <a class="reference internal" href="#id14">参见</a></span></p>
</div>
<p>要先声明的是：很难准确全面的把所有现代强化学习算法都列举出来，因为这些不适合用树形结构展示。同时，要把这么多内容放在一篇文章里，还要便于理解，必须省略掉一些更加先进的内容，例如探索（exploration），迁移学习（transfer learning），元学习（meta learning）等。</p>
<p>我们的目标是：</p>
<ul class="simple">
<li>只强调深度强化学习中最基础的设计选择：学习什么和怎么学习</li>
<li>揭示这些选择中的权衡</li>
<li>把其中特别优秀的算法介绍给大家</li>
</ul>
<div class="section" id="model-free-vs-model-based">
<h3>免模型学习（Model-Free） vs 有模型学习（Model-Based）<a class="headerlink" href="#model-free-vs-model-based" title="Permalink to this headline">¶</a></h3>
<p>强化学习算法最重要的区分点之一就是 <strong>智能体是否能完整了解或学习到所在环境的模型</strong>。 环境的模型是指一个预测状态转换和奖励的函数。</p>
<p>有模型学习最大的优势在于智能体能够 <strong>看的更远从而做出计划</strong>，走到每一步的时候，都提前尝试未来可能的选择，然后很清晰地从这些候选项中进行选择。智能体可以把提前计划得知的结果提取为一个学习到的策略。这其中最著名的例子就是 <a class="reference external" href="https://arxiv.org/abs/1712.01815">AlphaZero</a>。这个方法奏效的话，可以大幅度提升采样效率 —— 相对于那些没有模型的方法。</p>
<p>有模型学习最大的缺点就是智能体往往不能获得环境的真实模型。如果智能体想在一个场景下使用模型，那它必须完全从经验中学习，这会带来很多挑战。最大的挑战就是，智能体探索出来的模型和真实模型之间存在的误差，会导致智能体在学习到的模型中表现很好，但是在真实的环境中表现得不好（甚至很差）。从本质上讲，基于模型的学习非常困难，即使你愿意花费大量的时间和计算力，最终的结果也可能达不到预期的效果。</p>
</div>
<div class="section" id="id4">
<h3>要学习什么<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>另一个强化学习算法重要的区分点是 <strong>要学习什么</strong>。常提到的主题包括：</p>
<ul class="simple">
<li>策略，不管是随机的还是确定性的</li>
<li>行动奖励函数（Q函数）</li>
<li>值函数</li>
<li>环境模型</li>
</ul>
<div class="section" id="id5">
<h4>免模型学习中要学习什么<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<p>有两种用来表示和训练免模型学习强化学习算法的方式：</p>
<p><strong>策略优化（Policy Optimization）</strong>：这个系列的方法可以表示为 <img class="math" src="../_images/math/8bc2ffb416e6b729009dac19bf9efe3c144cc2ac.svg" alt="\pi_{\theta}(a|s)"/> 。 它们直接对目标函数进行梯度下降进行优化，或者间接地，对目标函数的局部近似函数进行优化。优化基本都是基于 <strong>同策路</strong> 的，也就是说每一步更新只会用到最新的策略执行时采集到的数据。策略优化也包括学习出 <img class="math" src="../_images/math/9810155b30a8fc71eaed8c6f0f62a228c21f11ed.svg" alt="V_{\phi}(s)"/> ，作为 <img class="math" src="../_images/math/3c343749cc2f7804a670b618d03d056d7e35b5eb.svg" alt="V^{\pi}(s)"/> 的近似，从而知道如何更新策略。</p>
<p>基于策略优化的方法举例：</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1602.01783">A2C / A3C</a>, 通过梯度下降直接最大化性能</li>
<li><a class="reference external" href="https://arxiv.org/abs/1707.06347">PPO</a>, 不直接通过最大化性能更新，而是最大化 <em>目标估计</em> 函数，这个函数是对目标函数math:<a href="#id6"><span class="problematic" id="id7">`</span></a>J(pi_{theta})`的近似估计。</li>
</ul>
<p><strong>Q-Learning</strong></p>
<div class="math">
<p><img src="../_images/math/c6b731255918704ea4b9dd46d1ac8a098b476a92.svg" alt="a(s) = \arg \max_a Q_{\theta}(s,a)."/></p>
</div><p>基于Q-Learning优化的方法</p>
<ul class="simple">
<li><a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DQN</a>, 一个让深度强化学习得到发展的经典方法</li>
<li>and <a class="reference external" href="https://arxiv.org/abs/1707.06887">C51</a>, a variant that learns a distribution over return whose expectation is <img class="math" src="../_images/math/27e0f032982b38571547cc19a81946ebdb69e1b9.svg" alt="Q^*"/>.</li>
<li>以及 <a class="reference external" href="https://arxiv.org/abs/1707.06887">C51</a>, 学习一个关于回报的分布函数，其期望是 <img class="math" src="../_images/math/27e0f032982b38571547cc19a81946ebdb69e1b9.svg" alt="Q^*"/></li>
</ul>
<p><strong>策略优化和Q-Learning的比较</strong> 策略优化的主要优势是这类方法很稳定很可靠，从直觉上将，你是直接在优化你想要的东西。Q-learning方法通过训练 <img class="math" src="../_images/math/cd8d851b25e574efd2664f88ccddda3e165cb693.svg" alt="Q_{\theta}"/> 以满足自我一致，间接地优化智能体的表现。这种方法有很多失败的情况，所以相对来说没有那么稳定。<a class="footnote-reference" href="#id9" id="id8">[1]</a> 但是，Q-learning有效的时候能获得更好的采样效率，因为它们能够比策略优化更加有效地重用数据。</p>
<p><strong>策略优化和Q-learning的融合方法</strong> 意外的是，策略优化和Q-learning并不是不能兼容的（在某些场景下，它们两者是 <a class="reference external" href="https://arxiv.org/abs/1704.06440">等价的</a> ），也有很多算法介于两种极端之间。这个范围之类的算法能够很好的平衡好两者之间的优点和缺点，比如说：</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1509.02971">DDPG</a> ，同时学习确定性策略和Q-函数，并通过彼此互相提升</li>
<li><a class="reference external" href="https://arxiv.org/abs/1801.01290">SAC</a> ，使用随机策略、熵正则化和一些其它技巧来稳定学习并在标准基准上得分高于DDPG的变种</li>
</ul>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id8">[1]</a></td><td>关于更多Q-learning方法如何失败的，参见： 1) 经典论文 <a class="reference external" href="http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf">Tsitsiklis and van Roy</a>, 2) 最近的文章 <a class="reference external" href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">review by Szepesvari</a> (在 4.3.2章节)  3) <a class="reference external" href="https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view">Sutton and Barto</a> 的第11章节，尤其是 11.3 (on &#8220;the deadly triad&#8221; of function approximation, bootstrapping, and off-policy data, together causing instability in value-learning algorithms).</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="id11">
<h3>有模型强化学习要学习什么<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>不同于免模型学习，有模型学习方法不是很好分类：很多方法之间都会有交叉。我们会列举一些例子，当然肯定不够详尽，覆盖不到全部。在这些例子里面， <strong>模型</strong> 有么已知要么可以学习到。</p>
<p><strong>背景：纯规划</strong> 这个最基础的方法，从不用表示策略，而是纯使用计划技术来选择行动，比如 <a class="reference external" href="https://en.wikipedia.org/wiki/Model_predictive_control">模型预测控制</a> (model-predictive control, MPC)。在模型预测控制中，智能体每次观察环境的时候，都会计算出一个对于当前模型最优的计划，这里的计划指的是未来一个固定时间段内，智能体会采取的行动。（超过视野的未来奖励可以通过）</p>
<p><strong>Background: Pure Planning.</strong> The most basic approach <em>never</em> explicitly represents the policy, and instead, uses pure planning techniques like <a href="#id34"><span class="problematic" id="id35">`model-predictive control`_</span></a> (MPC) to select actions. In MPC, each time the agent observes the environment, it computes a plan which is optimal with respect to the model, where the plan describes all actions to take over some fixed window of time after the present. (Future rewards beyond the horizon may be considered by the planning algorithm through the use of a learned value function.) The agent then executes the first action of the plan, and immediately discards the rest of it. It computes a new plan each time it prepares to interact with the environment, to avoid using an action from a plan with a shorter-than-desired planning horizon.</p>
<ul class="simple">
<li><a class="reference external" href="https://sites.google.com/view/mbmf">MBMF</a></li>
</ul>
<p><strong>Expert Iteration.</strong> A straightforward follow-on to pure planning involves using and learning an explicit representation of the policy, <img class="math" src="../_images/math/8bc2ffb416e6b729009dac19bf9efe3c144cc2ac.svg" alt="\pi_{\theta}(a|s)"/>. The agent uses a planning algorithm (like Monte Carlo Tree Search) in the model, generating candidate actions for the plan by sampling from its current policy. The planning algorithm produces an action which is better than what the policy alone would have produced, hence it is an &#8220;expert&#8221; relative to the policy. The policy is afterwards updated to produce an action more like the planning algorithm&#8217;s output.</p>
<p><strong>专家迭代</strong> 一个基于纯计划直接前向 <img class="math" src="../_images/math/8bc2ffb416e6b729009dac19bf9efe3c144cc2ac.svg" alt="\pi_{\theta}(a|s)"/>
* <a class="reference external" href="https://arxiv.org/abs/1705.08439">ExIt</a> 算法用这种算法训练深层神经网络来玩 Hex
* <a class="reference external" href="https://arxiv.org/abs/1712.01815">AlphaZero</a> 是这种方法的另一个例子</p>
<p><strong>Data Augmentation for Model-Free Methods.</strong> Use a model-free RL algorithm to train a policy or Q-function, but either 1) augment real experiences with fictitious ones in updating the agent, or 2) use <em>only</em> fictitous experience for updating the agent.
<strong>免模型方法的数据增强</strong> 使用免模型算法来训练策略或者 Q 函数，</p>
<ul class="simple">
<li>See <a class="reference external" href="https://arxiv.org/abs/1803.00101">MBVE</a> for an example of augmenting real experiences with fictitious ones.</li>
<li>See <a class="reference external" href="https://worldmodels.github.io/">World Models</a> for an example of using purely fictitious experience to train the agent, which they call &#8220;training in the dream.&#8221;</li>
</ul>
<p><strong>Embedding Planning Loops into Policies.</strong> Another approach embeds the planning procedure directly into a policy as a subroutine&#8212;so that complete plans become side information for the policy&#8212;while training the output of the policy with any standard model-free algorithm. The key concept is that in this framework, the policy can learn to choose how and when to use the plans. This makes model bias less of a problem, because if the model is bad for planning in some states, the policy can simply learn to ignore it.</p>
<ul class="simple">
<li>See <a class="reference external" href="https://arxiv.org/abs/1707.06203">I2A</a> for an example of agents being endowed with this style of imagination.</li>
</ul>
</div>
</div>
<div class="section" id="id13">
<h2><a class="toc-backref" href="#id33">分类中提到的算法链接</a><a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h2>
<span class="target" id="id14"></span><table class="docutils footnote" frame="void" id="id15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="https://arxiv.org/abs/1602.01783">A2C / A3C</a> (Asynchronous Advantage Actor-Critic): Mnih et al, 2016</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id16" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06347">PPO</a> (Proximal Policy Optimization): Schulman et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id17" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="https://arxiv.org/abs/1502.05477">TRPO</a> (Trust Region Policy Optimization): Schulman et al, 2015</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id18" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td><a class="reference external" href="https://arxiv.org/abs/1509.02971">DDPG</a> (Deep Deterministic Policy Gradient): Lillicrap et al, 2015</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id19" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.09477">TD3</a> (Twin Delayed DDPG): Fujimoto et al, 2018</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td><a class="reference external" href="https://arxiv.org/abs/1801.01290">SAC</a> (Soft Actor-Critic): Haarnoja et al, 2018</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td><a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DQN</a> (Deep Q-Networks): Mnih et al, 2013</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[9]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06887">C51</a> (Categorical 51-Atom DQN): Bellemare et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id23" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[10]</td><td><a class="reference external" href="https://arxiv.org/abs/1710.10044">QR-DQN</a> (Quantile Regression DQN): Dabney et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id24" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[11]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.01495">HER</a> (Hindsight Experience Replay): Andrychowicz et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id25" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[12]</td><td><a class="reference external" href="https://worldmodels.github.io/">World Models</a>: Ha and Schmidhuber, 2018</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id26" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[13]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06203">I2A</a> (Imagination-Augmented Agents): Weber et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id27" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[14]</td><td><a class="reference external" href="https://sites.google.com/view/mbmf">MBMF</a> (Model-Based RL with Model-Free Fine-Tuning): Nagabandi et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id28" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[15]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.00101">MBVE</a> (Model-Based Value Expansion): Feinberg et al, 2018</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[16]</td><td><a class="reference external" href="https://arxiv.org/abs/1712.01815">AlphaZero</a>: Silver et al, 2017</td></tr>
</tbody>
</table>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rl_intro3.html" class="btn btn-neutral float-right" title="第三部分：策略优化介绍" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="rl_intro.html" class="btn btn-neutral" title="第一部分：强化学习中的核心概念" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>